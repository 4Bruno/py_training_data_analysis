{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "957506e8-8e28-4c79-8ba3-a703eb841dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from numpy.random import Generator as gen\n",
    "from numpy.random import PCG64 as pcg\n",
    "import matplotlib.pyplot as plt\n",
    "from  datetime import datetime\n",
    "\n",
    "np.set_printoptions(suppress=True, linewidth=100, precision=2)\n",
    "\n",
    "DIR_RESOURCES = 'data_analyst\\Resources\\Data'\n",
    "dir_sources = os.path.join(os.getcwd(),DIR_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f45b9291-a1ac-48cd-a34a-f1d90cdb5941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_data_dictionary = np.genfromtxt(os.path.join(dir_sources,'loan-data-dictionary.xlsx'))\n",
      "loan_data = np.genfromtxt(os.path.join(dir_sources,'loan-data.csv'))\n",
      "loan_EUR_USD = np.genfromtxt(os.path.join(dir_sources,'loan_EUR-USD.csv'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_up_filename(name):\n",
    "    chars_to_replace = '-* '\n",
    "    trans_table = str.maketrans(chars_to_replace,'_' * len(chars_to_replace))\n",
    "    return os.path.splitext(name.strip().translate(trans_table))[0]\n",
    "# use it only for debugging and setting up project    \n",
    "# some files might require manual manipulation\n",
    "[print(f'{clean_up_filename(f)} = np.genfromtxt(os.path.join(dir_sources,\\'{f}\\'))') for f in os.listdir(dir_sources) if 'loan' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb716d59-7fec-4767-8531-a734e0ce2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleanerColumn:\n",
    "    def __init__(self, name, data , index, dtype):\n",
    "        self.index = index\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def unique(self,return_counts=False):\n",
    "        return np.unique(self.data,return_counts=return_counts)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (f'{col.dtype} with name {col.name} and index {col.index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf7b2fa-c45a-4e25-8e58-72e2e8f1a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self) -> DataCleanerColumn:\n",
    "        while self._iter_index_type < len(self.supported_dtypes_list):\n",
    "            #print(self._iter_index_type,self._iter_index_column)\n",
    "            dtype = self.supported_dtypes_list[self._iter_index_type]\n",
    "            shp = self._data[dtype][2].shape\n",
    "            dim = self._data[dtype][2].ndim\n",
    "            has_any = (shp[0] > 0)\n",
    "            is_one_dimension_and_zero = (self._iter_index_column == 0)\n",
    "            is_2d_and_index_within_bounds = ((dim > 1) and (self._iter_index_column < shp[1]))\n",
    "            if has_any and (is_one_dimension_and_zero or is_2d_and_index_within_bounds):\n",
    "                col = self[dtype,self._iter_index_column]\n",
    "                self._iter_index_column += 1\n",
    "                return col\n",
    "            #print(self._iter_index_type)\n",
    "            self._iter_index_column = 0\n",
    "            self._iter_index_type += 1\n",
    "\n",
    "        self._iter_index_column = 0\n",
    "        self._iter_index_type = 0\n",
    "        raise StopIteration\n",
    "\n",
    "    def __show_basic_info(self):\n",
    "        # method acts weird, I am not aware of the shunanigans here\n",
    "        self._iter_index_type = 0\n",
    "        self._iter_index_column = 0\n",
    "        for col in self:\n",
    "            print(col)\n",
    "                \n",
    "    def __init__(self,file_path,delimiter,generate=False,restore=False,usecols=None):\n",
    "        self.supported_dtypes_list = [\n",
    "            float,\n",
    "            str,\n",
    "            np.datetime64,\n",
    "            int\n",
    "        ]\n",
    "        \n",
    "        self._iter_index_type = 0\n",
    "        self._iter_index_column = 0\n",
    "        \n",
    "        self._data = {}\n",
    "        for dtype in self.supported_dtypes_list:\n",
    "            self._data[dtype] = [\n",
    "                                    np.array([],dtype=int),\n",
    "                                    np.array([],dtype=str),\n",
    "                                    np.array([],dtype=dtype)\n",
    "                                ]\n",
    "            \n",
    "        self.file_path = file_path\n",
    "        self.file_delimiter = delimiter\n",
    "        \n",
    "        if (generate and restore):\n",
    "            raise Exception(\"Both kwargs 'generate' and 'restore' can't be assigned to True\")\n",
    "        if generate:\n",
    "            self.gen_data(usecols)\n",
    "        if restore:\n",
    "            self.restore(file_path)\n",
    "\n",
    "    def restore(self,name):\n",
    "        data = np.load(name)\n",
    "        for dtype in self.supported_dtypes_list:\n",
    "            base_name = str(dtype).strip().replace(' ','').replace('\\'','')\n",
    "            self._data[dtype][0] = data[base_name + '_column_original_index']\n",
    "            self._data[dtype][1] = data[base_name + '_header_name']\n",
    "            self._data[dtype][2] = data[base_name + '_data']\n",
    "            \n",
    "    def export(self,name):\n",
    "        dict_arrays = {}\n",
    "        for dtype in self.supported_dtypes_list:\n",
    "            base_name = str(dtype).strip().replace(' ','').replace('\\'','')\n",
    "            dict_arrays[base_name + '_column_original_index'] = self._data[dtype][0]\n",
    "            dict_arrays[base_name + '_header_name'] = self._data[dtype][1]\n",
    "            dict_arrays[base_name + '_data'] = self._data[dtype][2]\n",
    "        print(dict_arrays.keys())\n",
    "        np.savez(name,**dict_arrays)\n",
    "\n",
    "    def rename_column(self, dtype, index, new_name):\n",
    "        self._data[dtype][1][index] = new_name\n",
    "        \n",
    "    def delete_column(self, dtype, index):\n",
    "        self._data[dtype][0] = np.delete(self._data[dtype][0],index,axis=None)\n",
    "        self._data[dtype][1] = np.delete(self._data[dtype][1],index,axis=None)\n",
    "        self._data[dtype][2] = np.delete(self._data[dtype][2],index,axis=1)\n",
    "\n",
    "    def add_column(self, dtype, nparray, ori_index, name):\n",
    "        self._data[dtype][0] = np.append(self._data[dtype][0], ori_index)\n",
    "        self._data[dtype][1] = np.append(self._data[dtype][1], name)\n",
    "        try:\n",
    "            self._data[dtype][2] = np.column_stack([self._data[dtype][2],nparray]).astype(dtype=dtype)\n",
    "        except Exception as e:\n",
    "            if self._data[dtype][2].size == 0:\n",
    "                self._data[dtype][2] = nparray.copy().astype(dtype=dtype)\n",
    "            else:\n",
    "                raise e\n",
    "                \n",
    "    def move_column(self, from_dtype, from_index, to_dtype):\n",
    "        ori_index = self._data[from_dtype][0][from_index]\n",
    "        from_name = self._data[from_dtype][1][from_index]\n",
    "        from_data = self._data[from_dtype][2][:,from_index]\n",
    "\n",
    "        self.add_column(to_dtype, from_data, ori_index, from_name)\n",
    "        self.delete_column(from_dtype,from_index)\n",
    "\n",
    "    def try_datetime(self, nparray, datetime_format):\n",
    "        # datetime lib handles more format options like:\n",
    "        # '%b-%y' (Mmm-YY : May-15)\n",
    "        unique_dates = np.unique(nparray)\n",
    "        array_dates = [np.datetime64('NaT') \n",
    "                       if date_str == '' else np.datetime64(datetime.strptime(date_str,datetime_format)) \n",
    "                       for date_str in unique_dates]\n",
    "        cpy = nparray.copy()\n",
    "        for i in range(unique_dates.shape[0]):\n",
    "            cpy[cpy == unique_dates[i]] = array_dates[i]\n",
    "        # now is safe to overwrite\n",
    "        nparray[:] = cpy[:]\n",
    "        return cpy\n",
    "\n",
    "    def get_type_data(self, dtype):\n",
    "        return self._data[dtype][2]\n",
    "        \n",
    "    def __getitem__(self, key) -> DataCleanerColumn:\n",
    "        dtype,index = key\n",
    "        name = self._data[dtype][1][index]\n",
    "        try:\n",
    "            data = self._data[dtype][2][:,index]\n",
    "        except IndexError as e:\n",
    "            data = self._data[dtype][2][:]\n",
    "        return DataCleanerColumn(name, data, index, dtype)\n",
    "    \n",
    "    def gen_data(self,usecols=None):\n",
    "        data_nan = np.genfromtxt(self.file_path,\n",
    "                                 delimiter=self.file_delimiter,\n",
    "                                 skip_header=True,\n",
    "                                 usecols=usecols)\n",
    "        # first pass\n",
    "        # open only numeric data\n",
    "        # calculate max val as placeholder for NaN\n",
    "        # identify non-numeric columns (where calculated mean is NaN)\n",
    "        temporary_max_val_plus_one = np.nanmax(data_nan) + 1\n",
    "        temp_mean = np.atleast_1d(np.nanmean(data_nan,axis=0))\n",
    "        \n",
    "        self.temp_stats = np.array([np.atleast_1d(np.nanmin(data_nan,axis = 0)),\n",
    "                                  temp_mean,\n",
    "                                  np.atleast_1d(np.nanmax(data_nan, axis = 0))])\n",
    "\n",
    "        self._data[str][0] = np.atleast_1d(np.argwhere(np.isnan(temp_mean)).squeeze())\n",
    "        self._data[float][0] = np.atleast_1d(np.argwhere(~np.isnan(temp_mean)).squeeze())\n",
    "\n",
    "        npa_usecols = np.atleast_1d(usecols)\n",
    "        if usecols is None:\n",
    "            npa_usecols = np.atleast_1d(np.arange(0, data_nan.shape[0]))\n",
    "            \n",
    "        # second pass\n",
    "        # STR\n",
    "        if self._data[str][0].shape[0] > 0:\n",
    "            # get source index of columns if usecols is passed\n",
    "            self._data[str][0] = npa_usecols[self._data[str][0]]\n",
    "            self._data[str][2] = np.genfromtxt(self.file_path,\n",
    "                                          delimiter=self.file_delimiter,\n",
    "                                          dtype=str,\n",
    "                                          usecols=self._data[str][0],\n",
    "                                          skip_header=True)\n",
    "\n",
    "        # FLOAT\n",
    "        if self._data[float][0].shape[0] > 0:\n",
    "            # get source index of columns if usecols is passed\n",
    "            self._data[float][0] = npa_usecols[self._data[float][0]]\n",
    "            self._data[float][2] = np.genfromtxt(self.file_path,\n",
    "                                          dtype='float',\n",
    "                                          delimiter=self.file_delimiter,\n",
    "                                          usecols=self._data[float][0],\n",
    "                                          skip_header=True,\n",
    "                                          filling_values=temporary_max_val_plus_one)\n",
    "\n",
    "        # this works because the above data contains all rows minus header\n",
    "        skip_footer_count = max(self._data[str][2].shape[0], self._data[float][2].shape[0])\n",
    "        headers_all = np.genfromtxt(self.file_path,\n",
    "                                    delimiter=self.file_delimiter,\n",
    "                                    dtype=str,\n",
    "                                    skip_footer=skip_footer_count)\n",
    "        \n",
    "        self._data[str][1] = headers_all[self._data[str][0]]\n",
    "        self._data[float][1]= headers_all[self._data[float][0]]\n",
    "\n",
    "    def __str__(self):\n",
    "        output = ''\n",
    "        for dtype in self.supported_dtypes_list:\n",
    "            output += f'\\nSUMMARY FOR {str(dtype)}'\n",
    "            output += f'\\nColumn original index:\\n{self._data[dtype][0]}'\n",
    "            output += f'\\nColumn name:\\n{self._data[dtype][1]}'\n",
    "            output += f'\\nData:\\n{self._data[dtype][2]}\\n'\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f6b718-9046-485c-9c53-cdb1f45054e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8236\\1876882789.py:142: RuntimeWarning: Mean of empty slice\n",
      "  temp_mean = np.atleast_1d(np.nanmean(data_nan,axis=0))\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8236\\1876882789.py:144: RuntimeWarning: All-NaN slice encountered\n",
      "  self.temp_stats = np.array([np.atleast_1d(np.nanmin(data_nan,axis = 0)),\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8236\\1876882789.py:146: RuntimeWarning: All-NaN slice encountered\n",
      "  np.atleast_1d(np.nanmax(data_nan, axis = 0))])\n"
     ]
    }
   ],
   "source": [
    "# Sources\n",
    "#df_xl = pd.read_excel(os.path.join(dir_sources,'loan-data-dictionary.xlsx'))\n",
    "loan_data_cleaner = DataCleaner(os.path.join(dir_sources, 'loan-data.csv') ,delimiter=';',generate=True)\n",
    "#loan_curr_cleaner = DataCleaner(os.path.join(dir_sources,'loan_EUR-USD.csv') ,delimiter=',',generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0af3a3-f719-48b8-a02d-d4f6cb32b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "#loan_data_cleaner.export('loan_data_imported')\n",
    "#loan_data_cleaner.restore('loan_data_imported.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c414940-fc8d-40f0-9f5d-a4255f3bf7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUMMARY FOR <class 'float'>\n",
      "Column original index:\n",
      "[ 0  2  4  6  7 13]\n",
      "Column name:\n",
      "['id' 'loan_amnt' 'funded_amnt' 'int_rate' 'installment' 'total_pymnt']\n",
      "Data:\n",
      "[[48010226.      35000.      35000.         13.33     1184.86     9452.96]\n",
      " [57693261.      30000.      30000.   68616520.        938.57     4679.7 ]\n",
      " [59432726.      15000.      15000.   68616520.        494.86     1969.83]\n",
      " ...\n",
      " [50415990.      10000.      10000.   68616520.   68616520.       2185.64]\n",
      " [46154151.   68616520.      10000.         16.55      354.3      3199.4 ]\n",
      " [66055249.      10000.      10000.   68616520.        309.97      301.9 ]]\n",
      "\n",
      "SUMMARY FOR <class 'str'>\n",
      "Column original index:\n",
      "[ 1  3  5  8  9 10 11 12]\n",
      "Column name:\n",
      "['issue_d' 'loan_status' 'term' 'grade' 'sub_grade' 'verification_status' 'url' 'addr_state']\n",
      "Data:\n",
      "[['May-15' 'Current' ' 36 months' ... 'Verified'\n",
      "  'https://www.lendingclub.com/browse/loanDetail.action?loan_id=48010226' 'CA']\n",
      " ['' 'Current' ' 36 months' ... 'Source Verified'\n",
      "  'https://www.lendingclub.com/browse/loanDetail.action?loan_id=57693261' 'NY']\n",
      " ['Sep-15' 'Current' ' 36 months' ... 'Verified'\n",
      "  'https://www.lendingclub.com/browse/loanDetail.action?loan_id=59432726' 'PA']\n",
      " ...\n",
      " ['Jun-15' 'Current' ' 36 months' ... 'Source Verified'\n",
      "  'https://www.lendingclub.com/browse/loanDetail.action?loan_id=50415990' 'CA']\n",
      " ['Apr-15' 'Current' ' 36 months' ... 'Source Verified'\n",
      "  'https://www.lendingclub.com/browse/loanDetail.action?loan_id=46154151' 'OH']\n",
      " ['Dec-15' 'Current' ' 36 months' ... ''\n",
      "  'https://www.lendingclub.com/browse/loanDetail.action?loan_id=66055249' 'IL']]\n",
      "\n",
      "SUMMARY FOR <class 'numpy.datetime64'>\n",
      "Column original index:\n",
      "[]\n",
      "Column name:\n",
      "[]\n",
      "Data:\n",
      "[]\n",
      "\n",
      "SUMMARY FOR <class 'int'>\n",
      "Column original index:\n",
      "[]\n",
      "Column name:\n",
      "[]\n",
      "Data:\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(loan_data_cleaner)\n",
    "#loan_data_cleaner.export('loan_data_imported')\n",
    "#loan_data_cleaner = DataCleaner('loan_data_imported.npz' ,delimiter=';',restore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5d413a-5ea0-407f-8f27-8de4e3597d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2015-01-01T00:00:00.000000', '2015-02-01T00:00:00.000000', '2015-03-01T00:00:00.000000',\n",
       "       '2015-04-01T00:00:00.000000', '2015-05-01T00:00:00.000000', '2015-06-01T00:00:00.000000',\n",
       "       '2015-07-01T00:00:00.000000', '2015-08-01T00:00:00.000000', '2015-09-01T00:00:00.000000',\n",
       "       '2015-10-01T00:00:00.000000', '2015-11-01T00:00:00.000000', '2015-12-01T00:00:00.000000',\n",
       "                              'NaT'], dtype='datetime64[us]')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# issue_d\n",
    "loan_data_cleaner.try_datetime(loan_data_cleaner[str,0].data,'%b-%y')\n",
    "loan_data_cleaner.move_column(str, 0, np.datetime64)\n",
    "loan_data_cleaner[np.datetime64,0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b429f07-a3d9-4a4e-8e25-096672978e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1'], dtype='<U69')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loan_status column - translate to 0 good status, 1 bad status\n",
    "col = loan_data_cleaner[str,0]\n",
    "col.unique()\n",
    "# manual list of what we decide is bad status\n",
    "list_bad_status = ['', 'Charged Off', 'Default', 'Late (31-120 days)']\n",
    "col.data[:] = np.where(np.isin(col.data, list_bad_status), 1,0)\n",
    "col.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b414bc5b-0ece-4074-874b-98b3b56d79e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['36', '60'], dtype='<U69')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# term column\n",
    "col = loan_data_cleaner[str,1].data\n",
    "np.unique(col)\n",
    "col[:] = np.chararray.replace(col,'months','')\n",
    "col[:]  = np.chararray.strip(col)\n",
    "default_term = 60\n",
    "col[col == ''] = default_term\n",
    "loan_data_cleaner.rename_column(str, 1,'term_months')\n",
    "np.unique(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802b32de-f3c6-4b4a-9f78-96e66a711303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count unique values grade col [ 515 1632 2606 2766 1389  816  236   40]\n",
      "Count unique values sub_grade col [514 285 278 239 323 502 509 517 530 553 494 629 567 586 564 423 391 267 250 255 223 235 162 171\n",
      " 139 114  94  52  34  43  16  19  10   3   7   2]\n",
      "['' 'A1' 'A2' 'A3' 'A4' 'A5' 'B1' 'B2' 'B3' 'B4' 'B5' 'C1' 'C2' 'C3' 'C4' 'C5' 'D1' 'D2' 'D3' 'D4'\n",
      " 'D5' 'E1' 'E2' 'E3' 'E4' 'E5' 'F1' 'F2' 'F3' 'F4' 'F5' 'G1' 'G2' 'G3' 'G4' 'G5']\n",
      "[  9 285 278 239 323 592 509 517 530 553 633 629 567 586 564 577 391 267 250 255 288 235 162 171\n",
      " 139 160  94  52  34  43  24  19  10   3   7   5]\n"
     ]
    }
   ],
   "source": [
    "# grade column\n",
    "# we have sub-grade column with more details, delete this column\n",
    "# before, make sure any row with sub-grade empty, is filled with grade value\n",
    "col_g = loan_data_cleaner[str,2].data\n",
    "np.unique(col_g)\n",
    "col_sg = loan_data_cleaner[str,3].data\n",
    "print(f'Count unique values grade col {np.unique(col_g,return_counts=True)[1]}')\n",
    "print(f'Count unique values sub_grade col {np.unique(col_sg,return_counts=True)[1]}')\n",
    "# if sub_grade is empty, we will fill with grade + 5 (A5,B5,...)\n",
    "default_undefined_grade_index = np.array('5')\n",
    "col_sg[:] = np.where( (col_sg == '') & (col_g != ''), np.char.add(col_g, default_undefined_grade_index), col_sg)\n",
    "# delete grade column\n",
    "loan_data_cleaner.delete_column(str, 2)\n",
    "col_sg = loan_data_cleaner[str,2].data\n",
    "print(np.unique(col_sg,return_counts=True)[0])\n",
    "print(np.unique(col_sg,return_counts=True)[1])\n",
    "# any empty value in sub grades, we will fill it with arbitrary value\n",
    "filling_subgrade = 'Z1'\n",
    "col_sg[:] = np.where(col_sg == '', filling_subgrade, col_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f312794-e41b-4c1e-b842-bce278cee74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22',\n",
       "       '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36',\n",
       "       '4', '5', '6', '7', '8', '9'], dtype='<U69')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub-grade column\n",
    "col = loan_data_cleaner[str,2].data\n",
    "np.unique(col)\n",
    "if_value = ['A1', 'A2', 'A3', 'A4', 'A5', 'B1', 'B2', 'B3', 'B4', 'B5', 'C1', 'C2', 'C3', 'C4',\n",
    "       'C5', 'D1', 'D2', 'D3', 'D4', 'D5', 'E1', 'E2', 'E3', 'E4', 'E5', 'F1', 'F2', 'F3', 'F4',\n",
    "       'F5', 'G1', 'G2', 'G3', 'G4', 'G5']\n",
    "if_value.append(filling_subgrade)\n",
    "if_value.sort()\n",
    "set_value = [(i + 1) for i in range(len(if_value))]\n",
    "for i in range(len(if_value)):\n",
    "    col[col == if_value[i]] = set_value[i]\n",
    "col\n",
    "np.unique(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcb4702f-6d52-4f77-9eeb-41b48b736071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1'], dtype='<U69')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verification_status column\n",
    "col = loan_data_cleaner[str,3].data\n",
    "np.unique(col)\n",
    "unverified_values = ['', 'Not Verified']\n",
    "col[:] = np.where(np.isin(col,unverified_values),0,1)\n",
    "np.unique(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd680a56-33f8-490e-b201-658bfd0b6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url column - useless, loan_id has same values and the url is unique\n",
    "col = loan_data_cleaner[str,4].data\n",
    "np.unique(col)\n",
    "base_url = 'https://www.lendingclub.com/browse/loanDetail.action?loan_id='\n",
    "loan_data_cleaner.delete_column(str, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdea42b4-3cb8-441c-92f2-abb322fadd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc of records without data 0.05\n",
      "Records with '' value is the top 20.15%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1336,  777,  758,  690,  500,  389,  341,  321,  320,  312,  267,  261,  242,  222,  220,\n",
       "         216,  210,  201,  160,  156,  152,  148,  143,  143,  130,  119,  116,  108,  107,   84,\n",
       "          84,   83,   74,   74,   61,   58,   57,   49,   44,   40,   28,   27,   27,   27,   26,\n",
       "          25,   24,   17,   16,   10], dtype=int64),\n",
       " array([0.13, 0.08, 0.08, 0.07, 0.05, 0.04, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.02, 0.02, 0.02,\n",
       "        0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "        0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.  , 0.  , 0.  ]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# addr_state column pre-analysis\n",
    "# looking at the amount of missing values ('')\n",
    "# it's above 45 states, so any coefficient for small states will be altered by outliers\n",
    "# Instead we will group states by region (S,W,N,E)\n",
    "col = loan_data_cleaner[str,4].data\n",
    "idx = np.argwhere(col == '')\n",
    "s_name, s_count = np.unique(col,return_counts=True)\n",
    "s_count_sorted = np.argsort(-s_count)\n",
    "s_name[s_count_sorted],s_count[s_count_sorted]\n",
    "total_missing_info = s_count[np.argwhere(s_name == '')][0][0]\n",
    "print(f'Perc of records without data {total_missing_info / s_count.sum()}')\n",
    "print(f'Records with \\'\\' value is the top {round(np.percentile(s_count,total_missing_info / s_count.sum() * 100),2)}%')\n",
    "s_count[s_count_sorted],s_count[s_count_sorted] / s_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eab60a3-7dc9-48c6-b48a-825422e33e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add region column\n",
    "col = loan_data_cleaner[str,4].data\n",
    "np.unique(col)\n",
    "west = ['AK', 'CA', 'HI', 'NV', 'OR', 'WA']\n",
    "east = ['CT', 'DE', 'DC', 'FL', 'GA', 'ME', 'MD', 'MA', 'NH', 'NJ', 'NY', 'NC', 'PA', 'RI', 'SC',\n",
    "        'VT', 'VA', 'WV']\n",
    "north = ['ID', 'IL', 'IN', 'IA', 'KS', 'MI', 'MN', 'MO', 'MT', 'NE', 'ND', 'OH', 'SD', 'WI', 'WY']\n",
    "south = ['AL', 'AR', 'AZ', 'CO', 'KY', 'LA', 'MS', 'NM', 'OK', 'TN', 'TX', 'UT']\n",
    "all_states = [west, east, north, south]\n",
    "# region 0, means undefined (NULL)\n",
    "# region [1-4], W,E,N,S\n",
    "for idx, state in enumerate(all_states):\n",
    "    col[np.isin(col,state)] = (idx + 1)\n",
    "col[col == ''] = 0\n",
    "#np.unique(col_cpy)  \n",
    "loan_data_cleaner.rename_column(str, 4,'Region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac5e5c1-05eb-4e41-86bb-410dff38f490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 36, 13,  1,  1],\n",
       "       [ 0, 36,  5,  1,  2],\n",
       "       [ 0, 36, 10,  1,  2],\n",
       "       ...,\n",
       "       [ 0, 36,  5,  1,  1],\n",
       "       [ 0, 36, 17,  1,  3],\n",
       "       [ 0, 36,  4,  0,  3]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can all strings now be converted to INT?\n",
    "loan_data_cleaner._data[str][2].astype(dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4626847-a12c-4a36-be9b-b0034e44909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'loan_amnt' 'funded_amnt' 'int_rate' 'installment' 'total_pymnt']\n",
      "id - Ignored\n",
      "loan_amnt : 35000.0\n",
      "funded_amnt : 1000.0\n",
      "int_rate : 28.99\n",
      "installment : 1372.97\n",
      "total_pymnt : 41913.62\n"
     ]
    }
   ],
   "source": [
    "# for numeric values we need to decide how to fill the empty cells\n",
    "# max is not always preferred as it might be bad for the particular column\n",
    "# for instance, if you have a column with client max loan value and you set it to max\n",
    "# most likely isn't the best possible value. Instead, min value will be more appropiate for the unknown\n",
    "loan_data_cleaner.temp_stats.transpose()\n",
    "print(loan_data_cleaner._data[float][1])\n",
    "# we set for max for all except funded_amnt\n",
    "col_fill_method = [np.nanmax] * loan_data_cleaner._data[float][1].size\n",
    "col_fill_method[2] = np.nanmin\n",
    "col_fill_method[0] = None\n",
    "col_fill_method\n",
    "temp_stats_num = loan_data_cleaner.temp_stats[:,loan_data_cleaner._data[float][0]].transpose()\n",
    "max_val_plus_one = np.max(loan_data_cleaner.temp_stats[:,loan_data_cleaner._data[float][0]]) + 1\n",
    "for index, method in enumerate(col_fill_method):\n",
    "    col = loan_data_cleaner[float, index]\n",
    "    if method != None:\n",
    "        val = method(temp_stats_num[index])\n",
    "        print(f'{col.name} : {val}')\n",
    "        col.data[col.data == max_val_plus_one] = val\n",
    "    else:\n",
    "        print(f'{col.name} - Ignored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "759ece35-af7a-4209-9128-ae4d544af5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# currency exchnge eur_usd 12 months of data\n",
    "loan_curr_cleaner = DataCleaner(os.path.join(dir_sources,'loan_EUR-USD.csv') ,delimiter=',',generate=True,usecols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6661d75-c6c7-476e-a944-c28406711271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'> with name Close and index 0\n"
     ]
    }
   ],
   "source": [
    "for col in loan_curr_cleaner:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "510b01b7-8d14-434c-b42c-de1e35560bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = loan_data_cleaner[np.datetime64,0]\n",
    "dates_as_int = dates.data.astype('datetime64[M]').astype(int)\n",
    "temp = np.zeros_like(dates_as_int).astype(dtype='float')\n",
    "for m in range(12):\n",
    "    dte = (np.datetime64('2015-01-01','M') + m).astype(int)\n",
    "    temp[((dates_as_int - dte) == 0)] = loan_curr_cleaner[float,0].data[m]\n",
    "# use mean ex rate for NaT values    \n",
    "temp[temp == 0] = np.mean(loan_curr_cleaner[float,0].data)    \n",
    "np.unique(temp)\n",
    "loan_data_cleaner.add_column(float,temp,-1,'exchange_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70675c68-d347-43ce-8eb4-5156b59b8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything now as numeric data\n",
    "while(loan_data_cleaner._data[str][2].size > 0):\n",
    "    loan_data_cleaner.move_column(str,0,float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab6492ac-e88a-4401-aa4f-cd331baf2c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack header to cleaned data and save output\n",
    "#loan_data_cleaner.export('loan_data_imported')\n",
    "#loan_data_cleaner = DataCleaner('loan_data_imported.npz',delimiter=',',restore=True)\n",
    "data_clean_and_sorted = loan_data_cleaner.get_type_data(float)[np.argsort(loan_data_cleaner[float,0].data)]\n",
    "data_clean_and_sorted = np.vstack( [loan_data_cleaner._data[float][1], data_clean_and_sorted] )\n",
    "np.savetxt('loan_data_preprocessed.csv', data_clean_and_sorted, fmt = '%s', delimiter = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
